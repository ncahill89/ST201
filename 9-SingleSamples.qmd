---
title: "ST201 Data Analysis"
subtitle:  "Single Samples"
author: "Prof. Niamh Cahill (she/her)"
format: 
  revealjs:
    highlight-style: github
editor: visual
---

## The Central Limit Theorem and the Normal Distribution {style="font-size: 65%;"}

-   If you take repeated samples from a population and calculate their averages, then these averages will be normally distributed.

-   This is called the central limit theorem

**Example**

Consider the distribution of dice rolls:

-   Possible outcomes are 1,2,3,4,5,6 and they are all equally likely

-   The distribution is uniform


```{r}
# Set up the die faces
faces <- 1:6

# Define the "true" probabilities for each face (uniform distribution)
true_prob <- rep(1/6, length(faces))

# Plot the true uniform distribution
barplot(true_prob,
        names.arg = faces,
        main = "True Uniform Distribution of Die Rolls",
        xlab = "Die Face",
        ylab = "Probability",
        col = "lightgreen",
        ylim = c(0, 0.2))  # Set y-axis limit slightly above 1/6 for spacing

# Add grid lines for clarity
grid()

```

::: incremental
-   The mean is `r (1+6)/2`

-   The sd is `r round(sqrt(((6-1)^2)/12),2)`
:::

## The Central Limit Theorem and the Normal Distribution {style="font-size: 65%;"}

-   Let X be a random variable representing a dice roll.

-   Assume we have 100 samples of X and the distribution looks like this

```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: ""
#| output-location: column-fragment

set.seed(1025)
dice_samp <- sample(1:6,
                    size = 100,
                    replace = TRUE)
barplot(table(dice_samp))

```

::: incremental
-   The mean of the sample is `r round(mean(dice_samp),2)`

-   The sd of the sample is `r round(sd(dice_samp),2)`
:::

## The Central Limit Theorem and the Normal Distribution {style="font-size: 65%;"}

-   We know the true distribution is not normal.

-   We can see that the distribution of the samples is not normal.

-   But what does the distribution of sample means look like?

-   Let's take some 1000 random samples of dice rolls with a sample size 100 and calculate their means.

```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: ""
#| output-location: column-fragment

samp_means <- rep(NA, 1000)

for(i in 1:1000)
{
  sample <- sample(1:6,size = 100,replace = TRUE)
  samp_means[i] <- mean(sample)
}
hist(samp_means,
     xlab = "sample means", 
     main = "")
```

::: incremental
-   The mean of the sample means is `r round(mean(samp_means),2)`

-   The sd of the sample means is `r round(sd(samp_means),2)`
:::

## The Central Limit Theorem {style="font-size: 65%;"}

$$\bar{X}\sim N(\mu, \frac{\sigma}{\sqrt{n}})  \text{   approximately for large n}.$$

Using $s$ to estimate $\sigma$: \begin{eqnarray*} 
\bar{X} &&{\underset{\text{ approx}}{\sim}}  N(\mu, \frac{s}{\sqrt{n}}) \\
        \\
 \end{eqnarray*}

Standardising $\bar{x}$ we get:

```{=tex}
\begin{eqnarray*} 

\frac{\bar{X}-\mu}{s/\sqrt{n}}&&{\underset{\text{ approx}}{\sim}} N(0,1)
\end{eqnarray*}
```
-   The quantity $\frac{s}{\sqrt{n}}$ is called the **standard error** (SE) of $\bar{X}$.

## Confidence intervals for $\mu$ {style="font-size: 65%;"}

Consider that 95% of the area under the $N(0,1)$ curve lies in $\pm 1.96$.

What does this mean?

$$P(-1.96<\frac{\bar{X}-\mu}{s/\sqrt{n}}<1.96)=0.95$$

And,

$$P(\bar{X}-1.96\frac{s}{\sqrt{n}}<\mu <\bar{X}+1.96\frac{s}{\sqrt{n}})=0.95$$

i.e., with probability 0.95 $\mu$ is contained in $$\bar{X} \pm 1.96\frac{s}{\sqrt{n}}$$ which is called a **95% confidence interval** for $\mu$.

## What if we have a small sample size? {style="font-size: 65%;"}

If you draw a simple random sample of size n from a population that has an approximately a normal distribution with mean $\mu$ and unknown population standard deviation $\sigma$ then

$$\frac{\bar{X}-\mu}{s/\sqrt{n}} \sim t_{n-1}$$

\bigskip

The degrees of freedom, $\nu = n - 1$, come from the calculation of the sample standard deviation s. Because the sum of the deviations is zero, we can find the last deviation once we know the other n - 1 deviations. The other n -1 deviations can change or vary freely. We call the number n - 1 the degrees of freedom (df).

## Comparing the normal and t-distributions {style="font-size: 65%;"}

-   The t-distribution has "fatter tails" than the normal distribution

-   Below we see a standard normal density (i.e., X \~ N(0,1)) and overlayed is a t density with df = 5.

```{r, fig.height= 3.5, fig.width= 4, fig.align='center'}
x <- seq(-4,4,0.1)
norm_dens <- dnorm(x)

t_dens <- dt(x,5)

plot(x, norm_dens, 
     type = "l",
     ylab = "Probability Density")
lines(x,t_dens,col = "red")
```

## Comparing the normal and t-distributions {style="font-size: 65%;"}

-   As the degrees of freedom increases the t-distribution tends to the normal distribution.

-   Below we see a standard normal density (i.e., X \~ N(0,1)) and overlayed is a t density with df = 40.

```{r, fig.height= 3.5, fig.width= 4, fig.align='center'}
x <- seq(-4,4,0.1)
norm_dens <- dnorm(x)

t_dens <- dt(x,40)

plot(x, norm_dens, 
     type = "l",
     ylab = "Probability Density")
lines(x,t_dens,col = "red")
```

## Confidence intervals for $\mu$ {style="font-size: 65%;"}

Consider that 95% of the area under the $t_{n-1}$ curve lies in $\pm T_{n-1, \alpha/2}$.

Therefore, $$P(-T_{n-1, \alpha/2}<\frac{\bar{X}-\mu}{s/\sqrt{n}}<T_{n-1, \alpha/2})=0.95$$

And,

$$P(\bar{X}-T_{n-1, \alpha/2}\frac{s}{\sqrt{n}}<\mu <\bar{X}+T_{n-1, \alpha/2}\frac{s}{\sqrt{n}})=0.95$$

i.e., with probability 0.95 $\mu$ is contained in $$\bar{X} \pm T_{n-1, \alpha/2}\frac{s}{\sqrt{n}}$$ which is called a 95% confidence interval for $\mu$.

## Example {style="font-size: 65%;"}

A study is conducted to measure the grams of protein for a sample of energy bars. The label claims that the bars have 20 grams of protein. We want to know if the labels are correct or not.

The histogram of the data for 20 samples are given below:

```{r,include = FALSE, echo = FALSE}

set.seed(20940)
x <- round(rnorm(25, 20, 5),2)
x
```

```{r, fig.height= 3, fig.width= 4, fig.align='center'}
hist(x, main = "")
```

## Assessing Normality {style="font-size: 65%;"}

Visual inspection is useful...

```{r,echo = FALSE, fig.height= 3, fig.width= 4, fig.align='center'}
qqnorm(x, xlim= c(-3,3), ylim = c(10,40))
qqline(x)
```

..but subjective.

::: incremental
-   It’s possible to use a significance test comparing the sample distribution to a normal one in order to ascertain whether or not data show a serious deviation from normality.
:::

## Assessing Normality {style="font-size: 65%;"}

Shapiro-Wilk’s method is widely recommended for normality test and it is based on the correlation between the data and the corresponding normal scores.

$H_0:$ the data come from a normal distribution

$H_a:$ the data do not come from a normal distribution

```{r, echo = TRUE}
shapiro.test(x)
```

## An Alternative to using distributions {style="font-size: 60%;"}

Consider a situation where you want to know whether you should buy a franchise of the used car store Awesome Autos. As part of your planning, you’d like to know for how much an average car from Awesome Autos sells. In order to go through the example more clearly, let’s say that you are only able to randomly sample five cars from Awesome Auto. (If this were a real example, you would surely be able to take a much larger sample size, possibly even being able to measure the entire population!)

**Bootstrapping**

[![Bootstrap Example from Openintro](images/Screenshot%202024-11-18%20at%2010.28.16.png){width="700"}](https://openintro-ims.netlify.app/inference-one-mean)

## An Alternative to using distributions {style="font-size: 65%;"}

**Bootstrapping**

```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: ""
#| output-location: column-fragment


car_data <- c(18300,20100, 9600, 10700, 27000)

n_samps <- 1000
bootstrap_samp_means <- rep(NA, n_samps)

for(i in 1:n_samps)
{
  bootstrap_samp <- sample(car_data, replace = TRUE)
  bootstrap_samp_means[i] <- mean(bootstrap_samp)
}

hist(bootstrap_samp_means)
```

::: incremental

  - The histogram summarizes one thousand bootstrap samples of the bootstrap sample means.

-   The bootstrapped average car prices vary from about \$10,000 to \$25,000.

-   The bootstrap percentile confidence interval is found by locating the middle 90% (for a 90% confidence interval) or a 95% (for a 95% confidence interval) of the bootstrapped means.
:::

. . . 

```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: ""
#| output-location: column-fragment

quantile(bootstrap_samp_means, c(0.025,0.975))
```

